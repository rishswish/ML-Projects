{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1pnKJqRcEmL"
   },
   "source": [
    "#**Steps to build a CNN Model For Yawn Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\risha\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (22.9.24)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (1.49.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvbuHEMRcLvO"
   },
   "source": [
    "## Step 1: Import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VLe8B_vmeJlj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31we0WVhcb5l"
   },
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "tlZFhB4QcbDO"
   },
   "outputs": [],
   "source": [
    "def buildCNN(training_dataset, testing_dataset):\n",
    "  train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                    shear_range = 0.2,\n",
    "                                    zoom_range = 0.2,\n",
    "                                    horizontal_flip = True)\n",
    "\n",
    "  training_set = train_datagen.flow_from_directory(training_dataset,\n",
    "                                                  target_size = (64, 64),\n",
    "                                                  batch_size = 32,\n",
    "                                                  class_mode = 'binary')\n",
    "\n",
    "  test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "  test_set = test_datagen.flow_from_directory(testing_dataset,\n",
    "                                              target_size = (64, 64),\n",
    "                                              batch_size = 32,\n",
    "                                              class_mode = 'binary')\n",
    "  cnn = tf.keras.models.Sequential()\n",
    "\n",
    "  # Part 1 - Convolution\n",
    "  cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
    "\n",
    "  # Part 2 - Pooling\n",
    "  cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "  # Part 3 - Adding a second convolutional layer\n",
    "  cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "  cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "  # Part 4 - Flattening\n",
    "  cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "  # Part 5 - Full Connection\n",
    "  cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "\n",
    "  # Part 6 - Output Layer\n",
    "  cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "  # Part 7 - Compiling the CNN\n",
    "  cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "  print('model training')\n",
    "\n",
    "  cnn.fit(x = training_set, validation_data = test_set, epochs = 25)\n",
    "  print('model train')\n",
    "\n",
    "  cnn.save('yawn_model.h5')\n",
    "  \n",
    "  return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32RkjTKFckwN"
   },
   "source": [
    "##Step 3 : Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vnV1py3JeyUc"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15320/2452082048.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'training_set' is not defined"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDlYK8YjcrKZ"
   },
   "source": [
    "## Step 4 : Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "e3ZiDJaJcrtn"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15320/3580577127.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'training_set' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SG_bUgQcx69"
   },
   "source": [
    "##Step 5 : Saving the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpizHKfRc4JZ"
   },
   "outputs": [],
   "source": [
    "  cnn.save('yawn_model.h5')\n",
    "  return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0PViwUAc84R"
   },
   "source": [
    "#Step 6 : Making Code Modular\n",
    "## All we have to do is pass in the training / testing data paths and we get a trained model as output.\n",
    "## To do this, we simply wrap the entire build in a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_Oti6-Zgy9R"
   },
   "source": [
    "# **If you already have a Model, start from here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SA01dUx6dzd9"
   },
   "source": [
    "#Step 7 : Load Model And Make A Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6pGQsHpeHPe"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.load_model(\"yawnmod.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDIXVSZSfOfJ"
   },
   "source": [
    "## Code To Take a Picture In Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OllWRoQ8dzxO"
   },
   "outputs": [],
   "source": [
    "def takepic(filename):\n",
    "  from IPython.display import Image\n",
    "  try:\n",
    "    filename = take_photo(filename)\n",
    "    print('Saved to {}'.format(filename))\n",
    "    \n",
    "    # Show the image which was just taken.\n",
    "    display(Image(filename))\n",
    "  except Exception as err:\n",
    "    # Errors will be thrown if the user does not have a webcam or if they do not\n",
    "    # grant the page permission to access it.\n",
    "    print(str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "736g4ieHdmdC"
   },
   "outputs": [],
   "source": [
    "def take_photo(filename, quality=1):\n",
    "  from IPython.display import display, Javascript\n",
    "  from google.colab.output import eval_js\n",
    "  from base64 import b64decode\n",
    "\n",
    "  js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const capture = document.createElement('button');\n",
    "      capture.textContent = 'Capture';\n",
    "      div.appendChild(capture);\n",
    "\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "  display(js)\n",
    "  data = eval_js('takePhoto({})'.format(quality))\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  with open(filename, 'wb') as f:\n",
    "    f.write(binary)\n",
    "  return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E655-jMIfWdL"
   },
   "source": [
    "##Code To Predict Output Of Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JatEiLYIdG3Y"
   },
   "outputs": [],
   "source": [
    "def predict(img_path):\n",
    "  test_image = tf.keras.utils.load_img(img_path, target_size = (64, 64))\n",
    "  test_image = tf.keras.utils.img_to_array(test_image)\n",
    "  test_image = np.expand_dims(test_image, axis = 0)\n",
    "  result = cnn.predict(test_image/255.0)\n",
    "  print(result[0][0])\n",
    "  if(result > 0.5):\n",
    "    return \"Yawn\"\n",
    "  else:\n",
    "    return \"No yawn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "xorJgjB7AaW-",
    "outputId": "49f7063a-5874-4c6f-f638-7554c5a34c97"
   },
   "outputs": [],
   "source": [
    "takepic(\"photo1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "5IyEJWPHAf83",
    "outputId": "5ad1eedd-22d1-42b5-80b8-000703855222"
   },
   "outputs": [],
   "source": [
    "predict(\"photo1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "6LdKO2iKf2ST",
    "outputId": "071476f6-da5e-43ca-84c0-67f9b3ff989f"
   },
   "outputs": [],
   "source": [
    "takepic(\"photo2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "Pm2w-sHef6ih",
    "outputId": "17a1315c-17a9-47b0-ffee-360ea36fbe4d"
   },
   "outputs": [],
   "source": [
    "predict(\"photo2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2467 images belonging to 4 classes.\n",
      "Found 433 images belonging to 4 classes.\n",
      "model training\n",
      "Epoch 1/25\n",
      "78/78 [==============================] - 34s 431ms/step - loss: -28361.4434 - accuracy: 0.2481 - val_loss: -170929.5312 - val_accuracy: 0.2517\n",
      "Epoch 2/25\n",
      "78/78 [==============================] - 22s 285ms/step - loss: -1601222.6250 - accuracy: 0.2501 - val_loss: -4870824.0000 - val_accuracy: 0.2517\n",
      "Epoch 3/25\n",
      "78/78 [==============================] - 23s 297ms/step - loss: -16471613.0000 - accuracy: 0.2501 - val_loss: -33911528.0000 - val_accuracy: 0.2517\n",
      "Epoch 4/25\n",
      "78/78 [==============================] - 28s 361ms/step - loss: -77250616.0000 - accuracy: 0.2501 - val_loss: -128350600.0000 - val_accuracy: 0.2517\n",
      "Epoch 5/25\n",
      "78/78 [==============================] - 24s 308ms/step - loss: -234712384.0000 - accuracy: 0.2501 - val_loss: -349239264.0000 - val_accuracy: 0.2517\n",
      "Epoch 6/25\n",
      "78/78 [==============================] - 23s 294ms/step - loss: -555087424.0000 - accuracy: 0.2501 - val_loss: -756940352.0000 - val_accuracy: 0.2517\n",
      "Epoch 7/25\n",
      "78/78 [==============================] - 33s 421ms/step - loss: -1111789952.0000 - accuracy: 0.2501 - val_loss: -1451352960.0000 - val_accuracy: 0.2517\n",
      "Epoch 8/25\n",
      "78/78 [==============================] - 27s 342ms/step - loss: -1999595904.0000 - accuracy: 0.2501 - val_loss: -2498926336.0000 - val_accuracy: 0.2517\n",
      "Epoch 9/25\n",
      "78/78 [==============================] - 30s 391ms/step - loss: -3325652224.0000 - accuracy: 0.2501 - val_loss: -4024084224.0000 - val_accuracy: 0.2517\n",
      "Epoch 10/25\n",
      "78/78 [==============================] - 23s 297ms/step - loss: -5251240960.0000 - accuracy: 0.2501 - val_loss: -6186211328.0000 - val_accuracy: 0.2517\n",
      "Epoch 11/25\n",
      "78/78 [==============================] - 23s 292ms/step - loss: -7881509376.0000 - accuracy: 0.2501 - val_loss: -9075972096.0000 - val_accuracy: 0.2517\n",
      "Epoch 12/25\n",
      "78/78 [==============================] - 25s 316ms/step - loss: -11223855104.0000 - accuracy: 0.2501 - val_loss: -12737624064.0000 - val_accuracy: 0.2517\n",
      "Epoch 13/25\n",
      "78/78 [==============================] - 25s 326ms/step - loss: -15470837760.0000 - accuracy: 0.2501 - val_loss: -17427365888.0000 - val_accuracy: 0.2517\n",
      "Epoch 14/25\n",
      "78/78 [==============================] - 27s 350ms/step - loss: -20941389824.0000 - accuracy: 0.2501 - val_loss: -23089891328.0000 - val_accuracy: 0.2517\n",
      "Epoch 15/25\n",
      "78/78 [==============================] - 23s 297ms/step - loss: -27346714624.0000 - accuracy: 0.2501 - val_loss: -29941612544.0000 - val_accuracy: 0.2517\n",
      "Epoch 16/25\n",
      "78/78 [==============================] - 24s 313ms/step - loss: -35269160960.0000 - accuracy: 0.2501 - val_loss: -38159785984.0000 - val_accuracy: 0.2517\n",
      "Epoch 17/25\n",
      "78/78 [==============================] - 21s 270ms/step - loss: -44541059072.0000 - accuracy: 0.2501 - val_loss: -47760936960.0000 - val_accuracy: 0.2517\n",
      "Epoch 18/25\n",
      "78/78 [==============================] - 25s 316ms/step - loss: -55340118016.0000 - accuracy: 0.2501 - val_loss: -58859827200.0000 - val_accuracy: 0.2517\n",
      "Epoch 19/25\n",
      "78/78 [==============================] - 26s 335ms/step - loss: -67861499904.0000 - accuracy: 0.2501 - val_loss: -71695499264.0000 - val_accuracy: 0.2517\n",
      "Epoch 20/25\n",
      "78/78 [==============================] - 24s 304ms/step - loss: -81854382080.0000 - accuracy: 0.2501 - val_loss: -86168903680.0000 - val_accuracy: 0.2517\n",
      "Epoch 21/25\n",
      "78/78 [==============================] - 24s 312ms/step - loss: -98155569152.0000 - accuracy: 0.2501 - val_loss: -102850117632.0000 - val_accuracy: 0.2517\n",
      "Epoch 22/25\n",
      "78/78 [==============================] - 22s 285ms/step - loss: -116465164288.0000 - accuracy: 0.2501 - val_loss: -121731588096.0000 - val_accuracy: 0.2517\n",
      "Epoch 23/25\n",
      "78/78 [==============================] - 25s 323ms/step - loss: -137156804608.0000 - accuracy: 0.2501 - val_loss: -142176534528.0000 - val_accuracy: 0.2517\n",
      "Epoch 24/25\n",
      "78/78 [==============================] - 26s 335ms/step - loss: -159241142272.0000 - accuracy: 0.2501 - val_loss: -164478664704.0000 - val_accuracy: 0.2517\n",
      "Epoch 25/25\n",
      "78/78 [==============================] - 31s 396ms/step - loss: -183331471360.0000 - accuracy: 0.2501 - val_loss: -189935599616.0000 - val_accuracy: 0.2517\n",
      "model train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_path=\"C:/Users/risha/OneDrive/Desktop/Driver-Drowsiness-system/dataset_new/train\"\n",
    "test_ds_path=\"C:/Users/risha/OneDrive/Desktop/Driver-Drowsiness-system/dataset_new/test\"\n",
    "\n",
    "buildCNN(train_ds_path,test_ds_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
